{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data\n",
    "\n",
    "In this notebook we are going to prepare the data from the testset. The dataset to use is stored on the Githubpage in the data folder and is named ['testset_delpher.csv](https://github.com/KBNLresearch/delpher_demo/blob/main/Data/testset_delpher.csv). \n",
    "We add a 'bag of words', which we create from the original text, we extract the year and we make a dataframe with keywords. \n",
    "\n",
    "When you use a dataset downloaded from http://delpher_demo.kbresearch.nl/, the bag of words and the year are already provided, so that part of the code can be skipped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have not yet installed the following packages, please install them on your command line with the following code: \n",
    "```\n",
    "pip install pandas\n",
    "pip install Unidecode\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the necessary package\n",
    "import pandas as pd\n",
    "import re\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the functions that will be needed later on\n",
    "\n",
    "## Sorts the words in the original string alphabetically\n",
    "def sortedSentence(Sentence): \n",
    "    # Splitting the Sentence into words \n",
    "    words = Sentence.split(\" \") \n",
    "    # Sorting the words \n",
    "    words.sort() \n",
    "    # Making new Sentence by joining the sorted words \n",
    "    newSentence = \" \".join(words) \n",
    "    return newSentence \n",
    "\n",
    "## Searches for given words in a given text and returns a list with id and word\n",
    "def search(identifier, text, wordlist):\n",
    "    identifier = identifier\n",
    "    text = text\n",
    "    keywords = wordlist\n",
    "    ## Go through every word in a given dictionary\n",
    "    for word in keywords_search:\n",
    "        ## If the word is found, store the indicated value and the id in the list\n",
    "        if word in text:\n",
    "            keywords.append([identifier, keywords_search.get(word)])\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the dataset\n",
    "file = ## Set the folder and name of the file (including .csv) in quotes\n",
    "data = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check if everything is loaded correctly\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the bag of words\n",
    "\n",
    "A bag of words is basically the text of the article in seperated words with punctuation and capitals removed. When the words in the bag of words are alfabetically ordered, the copyright restrictions no longer apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, we create a new column which contains the article text with every word in lower case.\n",
    "data['bag_of_words'] = data['text'].str.lower()\n",
    "## Remove the newline characters \\r and \\n \n",
    "data['bag_of_words'] = data['bag_of_words'].replace(r'\\r', '')\n",
    "data['bag_of_words'] = data['bag_of_words'].replace(r'\\n', '')\n",
    "## Remove the punctuation from the text, replaces not (^) word characters or spaces with the empty string (underscores\n",
    "## will not be removed)\n",
    "data['bag_of_words'] = data['bag_of_words'].apply(lambda x: re.sub(r'[^\\w\\s]','',str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To make sure the copyright laws no longer apply, we place the words in alfabetical order\n",
    "data['bag_of_words'] = data['bag_of_words'].apply(lambda x: sortedSentence(x))\n",
    "## And remove potential duplicate spaces that are now stored as first entry in our list of words\n",
    "data['bag_of_words'] = data['bag_of_words'].str.lstrip(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the year\n",
    "\n",
    "Normally, you can use build-in Python datetime functions to extract the year. However, when there are dates that are too far in the past (older then  1674-02-01), this will not work. Therefore, two methods of extracting the year are provided. For the provided testset the built-in Python functions will raise an error so it is recommended to use the second method. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The datetime method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new column with a copy of the date and set this column to datetime\n",
    "## Note: for the provided dataset, this option will raise an error!\n",
    "data['year']= pd.to_datetime(data['date'])\n",
    "## Extract the year\n",
    "data['year'] = data['year'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The manual method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new column in which the date is split based on the seperator and take the first output.\n",
    "## Note: the test dataset has two types of date formats, e.g. '1865-04-28' and '1861/06/24 00:00:00'\n",
    "## To take care of that, we need two different string splits.\n",
    "data['year'] = data['date'].str.split(\"-\").str[0]\n",
    "data['year'] = data['year'].str.split(\"/\").str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the file with the bag of words and the year\n",
    "dest_file = ## Set the folder and name of the destination file (including .csv) in quotes\n",
    "data.to_csv(dest_path, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract keywords\n",
    "\n",
    "Now we are ready to extract some keywords from the bag of words. We store these keywords in a seperate frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  First, we create a dictionary in which we define which keywords we want to search for, and which value they\n",
    "## must be given in the frame. For example, in Dutch we have historical word variatons, so if we want the keyword\n",
    "## 'ziekte' we also have to search for 'sieckt' and 'siekte'. We search for the shortest meaningfull word, to make \n",
    "## sure we include every variation. We choose to set the keywords to the current variant. \n",
    "\n",
    "keywords_search = { 'ziekt': 'ziekte',\n",
    "                    'siekt': 'ziekte',\n",
    "                    'sieck': 'ziekte',\n",
    "                    'virus': 'virus',\n",
    "                    'vira' : 'virus',\n",
    "                    'viren': 'virus',\n",
    "                    'griep' : 'griep',\n",
    "                    'vaccin': 'vaccin',\n",
    "                    'besmetting': 'besmetting',\n",
    "                    'influenza' : 'influenza',\n",
    "                    'quarantaine' : 'quarantaine',\n",
    "                    'verspreiding' : 'verspreding',\n",
    "                    'verspreyding' : 'verspreiding',\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create an empty list in which the keywords will be stored\n",
    "keywords = []\n",
    "\n",
    "## Iterate through the frame and use the function defined above to store found words\n",
    "for index, row in data.iterrows():\n",
    "    identifier = row['identifier']\n",
    "    text = row['bag_of_words']\n",
    "    ## Use unidecode to remove accents and normalise words, so this won't disturb matching\n",
    "    text = unidecode(text)\n",
    "    search(identifier, text, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a list of all identifiers that have one or more  keywords\n",
    "identifiers = [item[0] for item in keywords]\n",
    "## And make this a set to remove duplicates\n",
    "identifiers = set(identifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We now have all the identifiers that have a keyword, but we want to give all the other identifiers the keyword\n",
    "## 'overig'(other) so we can see how many articles are not related to any of the given keywords\n",
    "for index, row in data.iterrows():\n",
    "    identifier = row['identifier']\n",
    "    if identifier not in identifiers:\n",
    "        keywords.append([identifier, 'overig'])               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Turn the list into a datafrrame\n",
    "keywords_frame = pd.DataFrame(keywords, columns=['identifier','keyword']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the dataframe\n",
    "dest_file_key = ## Set the folder and name of the destination file (including .csv) in quotes\n",
    "keywords_frame.to_csv(dest_path_key, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
